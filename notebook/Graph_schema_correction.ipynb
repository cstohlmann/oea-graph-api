{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Module Ingestion - Schema Correction\r\n",
        "\r\n",
        "This notebook demonstrates the utility of the OEA_py class notebook, while correcting module tables initially ingested as un-flattened, with incorrect column data types, and lacking unique primary keys.\r\n",
        "\r\n",
        "Tables are read from ```stage2/Ingested/graph_api/(beta or v1.0)``` and written out, with the corrected schema, to ```stage2/Ingested_Corrected/graph_api/(beta or v1.0)```\r\n",
        "\r\n",
        "The steps outlined below describe how this notebook is used to correct the Microsoft Graph Reports API module tables:\r\n",
        "- Set the workspace for where the table schemas are to be corrected. \r\n",
        "- 5 functions are defined and used:\r\n",
        "   1. **_correct_users_table**: flattens the users table.\r\n",
        "   2. **_correct_m365_table**: flattens, corrects some column dtypes, and adds a primary key per row for the m365_app_user_detail table.\r\n",
        "   3. **_correct_teams_table**: flattens, corrects some column dtypes, and adds a primary key per row some column dtypes for the teams_activity_user_detail table.\r\n",
        "   4. **_correct_meetings_table**: flattens, corrects some column dtypes, and adds a primary key per row for the meeting_attendance_report table.\r\n",
        "   5. **correct_graph_dataset**: extracts the names of all the folders currently stored in stage2/Ingested/graph_api, corrects the schema per table using the functions above, and overwrites the tables with the updated schemas."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workspace = 'dev'\r\n",
        "testdataSet = 'hed'"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run OEA_py"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) set the workspace (this determines where in the data lake you'll be writing to and reading from).\r\n",
        "# You can work in 'dev', 'prod', or a sandbox with any name you choose.\r\n",
        "# For example, Sam the developer can create a 'sam' workspace and expect to find his datasets in the data lake under oea/sandboxes/sam\r\n",
        "oea.set_workspace(workspace)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) schema correction, since Graph data initially landed is unstructured with nested arrays, incorrect column dtypes, and without primary keys for 3 of the 4 tables.\r\n",
        "def _correct_users_table(df):\r\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values')).select(\"exploded_values.*\")\r\n",
        "    return df_flat\r\n",
        "\r\n",
        "def _correct_m365_table(df):\r\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values'), 'rundate').select(\"exploded_values.*\", 'rundate')\r\n",
        "    df_flat = df_flat.withColumn('reportPeriod', F.explode(F.col('details').reportPeriod)) \\\r\n",
        "                    .withColumn('mobile', F.explode(F.col('details').mobile)) \\\r\n",
        "                    .withColumn('web', F.explode(F.col('details').web)) \\\r\n",
        "                    .withColumn('mac', F.explode(F.col('details').mac)) \\\r\n",
        "                    .withColumn('windows', F.explode(F.col('details').windows)) \\\r\n",
        "                    .withColumn('excel', F.explode(F.col('details').excel)) \\\r\n",
        "                    .withColumn('excelMobile', F.explode(F.col('details').excelMobile)) \\\r\n",
        "                    .withColumn('excelWeb', F.explode(F.col('details').excelWeb)) \\\r\n",
        "                    .withColumn('excelMac', F.explode(F.col('details').excelMac)) \\\r\n",
        "                    .withColumn('excelWindows', F.explode(F.col('details').excelWindows)) \\\r\n",
        "                    .withColumn('oneNote', F.explode(F.col('details').oneNote)) \\\r\n",
        "                    .withColumn('oneNoteMobile', F.explode(F.col('details').oneNoteMobile)) \\\r\n",
        "                    .withColumn('oneNoteWeb', F.explode(F.col('details').oneNoteWeb)) \\\r\n",
        "                    .withColumn('oneNoteMac', F.explode(F.col('details').oneNoteMac)) \\\r\n",
        "                    .withColumn('oneNoteWindows', F.explode(F.col('details').oneNoteWindows)) \\\r\n",
        "                    .withColumn('outlook', F.explode(F.col('details').outlook)) \\\r\n",
        "                    .withColumn('outlookMobile', F.explode(F.col('details').outlookMobile)) \\\r\n",
        "                    .withColumn('outlookWeb', F.explode(F.col('details').outlookWeb)) \\\r\n",
        "                    .withColumn('outlookMac', F.explode(F.col('details').outlookMac)) \\\r\n",
        "                    .withColumn('outlookWindows', F.explode(F.col('details').outlookWindows)) \\\r\n",
        "                    .withColumn('powerPoint', F.explode(F.col('details').powerPoint)) \\\r\n",
        "                    .withColumn('powerPointMobile', F.explode(F.col('details').powerPointMobile)) \\\r\n",
        "                    .withColumn('powerPointWeb', F.explode(F.col('details').powerPointWeb)) \\\r\n",
        "                    .withColumn('powerPointMac', F.explode(F.col('details').powerPointMac)) \\\r\n",
        "                    .withColumn('powerPointWindows', F.explode(F.col('details').powerPointWindows)) \\\r\n",
        "                    .withColumn('teams', F.explode(F.col('details').teams)) \\\r\n",
        "                    .withColumn('teamsMobile', F.explode(F.col('details').teamsMobile)) \\\r\n",
        "                    .withColumn('teamsWeb', F.explode(F.col('details').teamsWeb)) \\\r\n",
        "                    .withColumn('teamsMac', F.explode(F.col('details').teamsMac)) \\\r\n",
        "                    .withColumn('teamsWindows', F.explode(F.col('details').teamsWindows)) \\\r\n",
        "                    .withColumn('word', F.explode(F.col('details').word)) \\\r\n",
        "                    .withColumn('wordMobile', F.explode(F.col('details').wordMobile)) \\\r\n",
        "                    .withColumn('wordWeb', F.explode(F.col('details').wordWeb)) \\\r\n",
        "                    .withColumn('wordMac', F.explode(F.col('details').wordMac)) \\\r\n",
        "                    .withColumn('wordWindows', F.explode(F.col('details').wordWindows)) \\\r\n",
        "                    .drop('details')\r\n",
        "    # temp: add unique primary key per row, by combining UPNs with the date the report was generated\r\n",
        "    # this assumes every person has only one row per reportRefreshDate\r\n",
        "    df_flat = df_flat.withColumn('m365Activity_pk', F.concat(F.col('userPrincipalName'),F.lit('_'),F.col('reportRefreshDate')))\r\n",
        "\r\n",
        "    df_flat.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "    df_flat.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "    df_flat.select(F.col('lastActivationDate'), F.to_date(F.col('lastActivationDate'), 'yyyy-MM-dd'))\r\n",
        "    return df_flat\r\n",
        "\r\n",
        "def _correct_teams_table(df):\r\n",
        "    df_flat = df.select(F.explode('value').alias('exploded_values'), 'rundate').select(\"exploded_values.*\", 'rundate')\r\n",
        "    df_flat = df_flat.withColumn('assignedProducts', F.explode(F.col('assignedProducts')))\r\n",
        "    # convert duration to seconds only \r\n",
        "    # NOTE: The duration expression may have changed and this will need to be modified to accommodate any new duration formatting\r\n",
        "    df_flat = df_flat.withColumn(\r\n",
        "        'screenShareDuration', \r\n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "        F.coalesce(F.regexp_extract('screenShareDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "        ).withColumn(\r\n",
        "        'videoDuration', \r\n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "        F.coalesce(F.regexp_extract('videoDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "        ).withColumn(\r\n",
        "        'audioDuration', \r\n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)H', 1).cast('int'), F.lit(0)) * 3600 + \r\n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)M', 1).cast('int'), F.lit(0)) * 60 + \r\n",
        "        F.coalesce(F.regexp_extract('audioDuration', r'(\\d+)S', 1).cast('int'), F.lit(0))\r\n",
        "        )\r\n",
        "    # temp: add unique primary key per row, by combining UPNs with the date the report was generated\r\n",
        "    # this assumes every person has only one row per reportRefreshDate\r\n",
        "    df_flat = df_flat.withColumn('teamsActivity_pk', F.concat(F.col('userPrincipalName'),F.lit('_'),F.col('reportRefreshDate')))\r\n",
        "\r\n",
        "    df_flat.select(F.col('reportRefreshDate'), F.to_date(F.col('reportRefreshDate'), 'yyyy-MM-dd'))\r\n",
        "    df_flat.select(F.col('lastActivityDate'), F.to_date(F.col('lastActivityDate'), 'yyyy-MM-dd'))\r\n",
        "    return df_flat\r\n",
        "\r\n",
        "def _correct_meetings_table(df):\r\n",
        "    df_flat = df.select(\r\n",
        "        \"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\",\r\n",
        "        F.explode(\"attendanceRecords\").alias(\"attendanceRecordsExplode\"), 'rundate'\r\n",
        "        ).select(\"id\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "                \"attendanceRecordsExplode.*\", 'rundate')\r\n",
        "    df_flat = df_flat.withColumnRenamed(\"id\",\"meetingId\")\r\n",
        "    df_flat = df_flat.select(\r\n",
        "        \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "        \"totalAttendanceInSeconds\", \"role\", \"emailAddress\", \"attendanceIntervals\",\r\n",
        "        F.explode(F.array(\"identity\")).alias(\"identityExplode\"), 'rundate'\r\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\"attendanceIntervals\",\r\n",
        "            \"identityExplode.*\", 'rundate')\r\n",
        "\r\n",
        "    df_flat = df_flat.select(\r\n",
        "        \"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "        \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\r\n",
        "        \"displayName\", \"id\", \"tenantId\",\r\n",
        "        F.explode(\"attendanceIntervals\").alias(\"attendanceIntervalsExplode\"), 'rundate'\r\n",
        "        ).select(\"meetingId\", \"meetingEndDateTime\", \"meetingStartDateTime\", \"totalParticipantCount\", \r\n",
        "            \"totalAttendanceInSeconds\", \"role\", \"emailAddress\",\r\n",
        "            \"attendanceIntervalsExplode.*\",\r\n",
        "            \"displayName\", \"id\", \"tenantId\", 'rundate')\r\n",
        "    # clean up column names and timestamp types\r\n",
        "    df_flat = df_flat.withColumnRenamed(\"id\", \"userId\").withColumnRenamed(\"displayName\", \"userDisplayName\").withColumnRenamed(\"emailAddress\", \"userEmailAddress\") \\\r\n",
        "        .withColumnRenamed(\"totalAttendanceInSeconds\", \"totalAttendanceInSec\").withColumnRenamed(\"tenantId\", \"userTenantId\") \\\r\n",
        "        .withColumnRenamed(\"joinDateTime\", \"attendanceInterval_joinDateTime\").withColumnRenamed(\"leaveDateTime\", \"attendanceInterval_leaveDateTime\").withColumnRenamed(\"durationInSeconds\", \"attendanceInterval_durationInSec\")\r\n",
        "    df_flat = df_flat.withColumn('meetingStartDateTime', F.to_timestamp(F.col('meetingStartDateTime'))) \\\r\n",
        "                .withColumn('meetingEndDateTime', F.to_timestamp(F.col('meetingEndDateTime'))) \\\r\n",
        "                .withColumn('attendanceInterval_joinDateTime', F.to_timestamp(F.col('attendanceInterval_joinDateTime'))) \\\r\n",
        "                .withColumn('attendanceInterval_leaveDateTime', F.to_timestamp(F.col('attendanceInterval_leaveDateTime')))\r\n",
        "    # temp: add unique primary key per row, by combining meeting IDs with user IDs\r\n",
        "    # this assumes every person attending a meeting has only one attendance interval\r\n",
        "    df_flat = df_flat.withColumn('meetingUserId_pk', F.concat(F.col('meetingId'),F.col('userId')))\r\n",
        "    df_flat = df_flat.select(\r\n",
        "        'meetingUserId_pk','meetingId','meetingStartDateTime','meetingEndDateTime','totalParticipantCount','userId','userDisplayName','userEmailAddress',\r\n",
        "        'userTenantId','role','totalAttendanceInSec','attendanceInterval_joinDateTime','attendanceInterval_leaveDateTime','attendanceInterval_durationInSec','rundate'\r\n",
        "        )\r\n",
        "    return df_flat\r\n",
        "\r\n",
        "def correct_graph_dataset(tables_source, write_destination):\r\n",
        "    items = oea.get_folders(tables_source)\r\n",
        "    for item in items: \r\n",
        "        table_path = tables_source +'/'+ item\r\n",
        "        if item == 'metadata.csv':\r\n",
        "            logger.info('ignore metadata processing, since this is not a table to be ingested')\r\n",
        "        elif item == 'users':\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df_users = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = _correct_users_table(streaming_df_users)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' +item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the users table from: ' + table_path)\r\n",
        "        elif item == 'm365_app_user_detail':\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df_m365 = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = _correct_m365_table(streaming_df_m365)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the m365_app_user_detail table from: ' + table_path)\r\n",
        "        elif item == 'teams_activity_user_detail':\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df_teams = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = _correct_teams_table(streaming_df_teams)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the teams_activity_user_detail table from: ' + table_path)\r\n",
        "        elif item == 'meeting_attendance_report':\r\n",
        "            spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
        "            streaming_df_meetings = spark.readStream.format('delta').load(oea.to_url(table_path))\r\n",
        "            df_corrected = _correct_meetings_table(streaming_df_meetings)\r\n",
        "            query = df_corrected.writeStream.format('delta').outputMode('append').trigger(once=True).option('checkpointLocation', oea.to_url(table_path) + '/_checkpoints')\r\n",
        "            query = query.start(oea.to_url(write_destination + '/' + item))\r\n",
        "            query.awaitTermination() \r\n",
        "            logger.info('Successfully corrected the meeting_attendance_report table from: ' + table_path)\r\n",
        "        else:\r\n",
        "            logger.info('No defined function for table: ' + item)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if testdataSet == 'k12':\r\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/beta', 'stage2/Ingested_Corrected/graph_api/beta')\r\n",
        "    logger.info('Finished schema correction for Graph dataset')\r\n",
        "elif testdataSet == 'hed':\r\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/beta', 'stage2/Ingested_Corrected/graph_api/beta')\r\n",
        "    correct_graph_dataset('stage2/Ingested/graph_api/v1.0', 'stage2/Ingested_Corrected/graph_api/v1.0')\r\n",
        "    logger.info('Finished schema correction for Graph dataset')\r\n",
        "else:\r\n",
        "    logger.info('Unrecognized testdataSet - please choose either k12 or hed.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('delta').load(oea.to_url('stage2/Ingested_Corrected/graph_api/v1.0/meeting_attendance_report'), header='true')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": "Used for \"2_ingest_graph\" pipeline. Corrects the schema from Graph data originally landed - flattening the nested JSON arrays, and correcting column names/data types.",
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}