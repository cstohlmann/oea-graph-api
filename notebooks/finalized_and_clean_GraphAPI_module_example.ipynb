{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph API Module Example Notebook\r\n",
        "\r\n",
        "This notebook creates 3 tables (users, m365_app_user_detail and teams_acivity_user_details) into a new Spark database called graphapi. \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provision storage accounts\r\n",
        "\r\n",
        "The storage account variable has to be changed to the name of the storage account associated with your Azure resource group."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.window import Window\r\n",
        "\r\n",
        "\r\n",
        "# data lake and container information\r\n",
        "storage_account = 'stoeahybriddev2'\r\n",
        "use_test_env = False\r\n",
        "\r\n",
        "if use_test_env:\r\n",
        "    stage1np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage1np'\r\n",
        "    stage2np = 'abfss://test-env@' + storage_account + '.dfs.core.windows.net/stage2np'\r\n",
        "else:\r\n",
        "    stage1np = 'abfss://stage1np@' + storage_account + '.dfs.core.windows.net'\r\n",
        "    stage2np = 'abfss://stage2np@' + storage_account + '.dfs.core.windows.net'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium2",
              "session_id": 2,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T18:17:25.5045607Z",
              "session_start_time": "2021-10-05T18:17:25.5437191Z",
              "execution_start_time": "2021-10-05T18:19:43.2589715Z",
              "execution_finish_time": "2021-10-05T18:19:43.4061758Z"
            },
            "text/plain": "StatementMeta(medium2, 2, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Raw Data from Lake\r\n",
        "To ensure that that the right tables are loaded, confirm that the file paths match your data lake storage containers. \r\n",
        "\r\n",
        "The top code-block defines the schema of how each of the stage 1 JSON files are stored."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# schemas for each of the JSON files into tables\r\n",
        "user_schema = StructType(fields=[\r\n",
        "    StructField('value', ArrayType(\r\n",
        "        StructType([\r\n",
        "            StructField('surname', StringType(), False),\r\n",
        "            StructField('givenName', StringType(), False),\r\n",
        "            StructField('userPrincipalName', StringType(), False),\r\n",
        "            StructField('id', StringType(), False)\r\n",
        "        ])\r\n",
        "    ))\r\n",
        "])\r\n",
        "\r\n",
        "m365_app_user_details_schema = StructType(fields=[\r\n",
        "    StructField('value', ArrayType(\r\n",
        "        StructType([\r\n",
        "            StructField('reportRefreshDate', StringType(), False),\r\n",
        "            StructField('userPrincipalName', StringType(), False),\r\n",
        "            StructField('lastActivityDate', StringType(), False),\r\n",
        "            StructField('reportPeriod', StringType(), False),\r\n",
        "            StructField('excel', StringType(), False),\r\n",
        "            StructField('excelWeb', StringType(), False),\r\n",
        "            StructField('outlook', StringType(), False),\r\n",
        "            StructField('outlookWeb', StringType(), False),\r\n",
        "            StructField('powerPoint', StringType(), False),\r\n",
        "            StructField('powerPointWeb', StringType(), False),\r\n",
        "            StructField('teams', StringType(), False),\r\n",
        "            StructField('teamsWeb', StringType(), False),\r\n",
        "            StructField('word', StringType(), False),\r\n",
        "            StructField('wordWeb', StringType(), False),\r\n",
        "            StructField('details', StringType())\r\n",
        "        ])\r\n",
        "    ))\r\n",
        "])\r\n",
        "\r\n",
        "teams_activity_user_details_schema = StructType(fields=[\r\n",
        "    StructField('value', ArrayType(\r\n",
        "        StructType([\r\n",
        "            StructField('reportRefreshDate', StringType(), False),\r\n",
        "            StructField('reportPeriod', StringType(), False),\r\n",
        "            StructField('userPrincipalName', StringType(), False),\r\n",
        "            StructField('privateChatMessageCount', IntegerType(), False),\r\n",
        "            StructField('teamChatMessageCount', IntegerType(), False),\r\n",
        "            StructField('meetingsAttendedCount', IntegerType(), False),\r\n",
        "            StructField('meetingCount', IntegerType(), False),\r\n",
        "            StructField('audioDuration', StringType(), False),\r\n",
        "        ])\r\n",
        "    ))\r\n",
        "])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium2",
              "session_id": 2,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T18:19:17.489283Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T18:19:43.5244817Z",
              "execution_finish_time": "2021-10-05T18:19:43.68148Z"
            },
            "text/plain": "StatementMeta(medium2, 2, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load needed tables from JSON data lake storage\r\n",
        "dfUsersRaw = spark.read.format('json').load(f'{stage1np}/GraphAPI/users.json', schema=user_schema)\r\n",
        "dfM365UserActivityRaw = spark.read.format('json').load(f'{stage1np}/GraphAPI/m365_app_user_detail.json', schema=m365_app_user_details_schema)\r\n",
        "dfTeamsUserActivityRaw = spark.read.format('json').load(f'{stage1np}/GraphAPI/teams_activity_user_details.json', schema=teams_activity_user_details_schema)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium2",
              "session_id": 2,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T18:21:28.7392194Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T18:21:28.8461217Z",
              "execution_finish_time": "2021-10-05T18:21:29.3433888Z"
            },
            "text/plain": "StatementMeta(medium2, 2, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Users table\r\n",
        "Contains all users (students and teachers) at a school-system level\r\n",
        "\r\n",
        "** Databases and tables used: **\r\n",
        "\r\n",
        " - None \r\n",
        " \r\n",
        "**JSON files used:**\r\n",
        "\r\n",
        "- users.json\r\n",
        "\r\n",
        "**Database and table created:**\r\n",
        "\r\n",
        "1. Spark DB: graphapi\r\n",
        "- Table: users"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfUsersRaw = dfUsersRaw.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium2",
              "session_id": 2,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-10-05T18:49:26.035693Z",
              "session_start_time": null,
              "execution_start_time": "2021-10-05T18:49:26.1443762Z",
              "execution_finish_time": "2021-10-05T18:49:26.2941242Z"
            },
            "text/plain": "StatementMeta(medium2, 2, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Data Back to Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write back to the lake in stage 2 ds2_main directory\r\n",
        "dfUsersRaw.write.format('parquet').mode('overwrite').save(stage2np + '/GraphAPI/users')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load to Spark DB"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\r\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
        "def create_spark_db(db_name, source_path):\r\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.users\")\r\n",
        "    spark.sql(f\"create table if not exists {db_name}.users using PARQUET location '{source_path}/users'\")\r\n",
        "    \r\n",
        "create_spark_db('graphapi', stage2np + '/GraphAPI/users')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. M365_app_user_detail table\r\n",
        "Contains a sample m365 table to support data analysis in a Power BI dashboard.\r\n",
        "\r\n",
        "**Databases and tables used:**\r\n",
        "- None\r\n",
        "\r\n",
        "**JSON files used:**\r\n",
        "- m365_app_user_detail.json\r\n",
        "\r\n",
        "**Databases and tables created:**\r\n",
        "\r\n",
        "1. Spark DB: graphapi\r\n",
        "- Table: m365_app_user_detail"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfM365UserActivityRaw = dfM365UserActivityRaw.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a roundabout method (taking in details as a string rather than array), but nontheless - it works for now.\r\n",
        "\r\n",
        "# Isolate and allocate data from \"details\" to their respective columns\r\n",
        "splitDF = dfM365UserActivityRaw.withColumn('reportPeriod', split(col('details'), ',').getItem(0)).withColumn('outlook', split(col('details'), ',').getItem(5)).withColumn('word', split(col('details'), ',').getItem(6)).withColumn('excel', split(col('details'), ',').getItem(7)).withColumn('powerPoint', split(col('details'), ',').getItem(8)).withColumn('teams', split(col('details'), ',').getItem(10)).withColumn('outlookWeb', split(col('details'), ',').getItem(29)).withColumn('wordWeb', split(col('details'), ',').getItem(30)).withColumn('excelWeb', split(col('details'), ',').getItem(31)).withColumn('powerPointWeb', split(col('details'), ',').getItem(32)).withColumn('teamsWeb', split(col('details'), ',').getItem(34))\r\n",
        "splitDF = splitDF.drop('details')\r\n",
        "\r\n",
        "# Clean the data within each column, to remove excess string pieces\r\n",
        "splitDF = splitDF.withColumn('reportPeriod', regexp_replace('reportPeriod', '\"reportPeriod\":', '')).withColumn('excel', regexp_replace('excel','\"excel\":', '')).withColumn('excelWeb', regexp_replace('excelWeb','\"excelWeb\":', '')).withColumn('outlook', regexp_replace('outlook','\"outlook\":', '')).withColumn('outlookWeb', regexp_replace('outlookWeb','\"outlookWeb\":', '')).withColumn('powerPoint', regexp_replace('powerPoint','\"powerPoint\":', '')).withColumn('powerPointWeb', regexp_replace('powerPointWeb','\"powerPointWeb\":', '')).withColumn('teams', regexp_replace('teams','\"teams\":', '')).withColumn('teamsWeb', regexp_replace('teamsWeb','\"teamsWeb\":', '')).withColumn('word', regexp_replace('word','\"word\":', '')).withColumn('wordWeb', regexp_replace('wordWeb','\"wordWeb\":', ''))\r\n",
        "splitDF = splitDF.withColumn('reportPeriod', regexp_replace('reportPeriod', '\\W+', '')).withColumn('teamsWeb', regexp_replace('teamsWeb', '\\W+', ''))\r\n",
        "\r\n",
        "# Transform the datatypes of the columns to be accurate to what they represent\r\n",
        "splitDF = splitDF.withColumn('reportPeriod', col('reportPeriod').cast(\"int\")).withColumn('excel', col('excel').cast(\"boolean\")).withColumn('excelWeb', col('excelWeb').cast(\"boolean\")).withColumn('outlook', col('outlook').cast(\"boolean\")).withColumn('outlookWeb', col('outlookWeb').cast(\"boolean\")).withColumn('powerPoint', col('powerPoint').cast(\"boolean\")).withColumn('powerPointWeb', col('powerPointWeb').cast(\"boolean\")).withColumn('teams', col('teams').cast(\"boolean\")).withColumn('teamsWeb', col('teamsWeb').cast(\"boolean\")).withColumn('word', col('word').cast(\"boolean\")).withColumn('wordWeb', col('wordWeb').cast(\"boolean\"))\r\n",
        "\r\n",
        "display(splitDF.limit(10))\r\n",
        "#splitDF.printSchema\r\n",
        "#display(splitDF)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Data Back to Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write back to the lake in stage 2 ds2_main directory\r\n",
        "splitDF.write.format('parquet').mode('overwrite').save(stage2np + '/GraphAPI/m365_app_user_detail')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load to Spark DB"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\r\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
        "def create_spark_db(db_name, source_path):\r\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.m365_app_user_detail\")\r\n",
        "    spark.sql(f\"create table if not exists {db_name}.users using PARQUET location '{source_path}/m365_app_user_detail'\")\r\n",
        "    \r\n",
        "create_spark_db('graphapi', stage2np + '/GraphAPI/m365_app_user_detail')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Teams_activity_user_details table\r\n",
        "Contains a sample Teams table to support data analysis in a Power BI dashboard.\r\n",
        "\r\n",
        "**Databases and tables used:**\r\n",
        "- None\r\n",
        "\r\n",
        "**JSON files used:**\r\n",
        "- teams_activity_user_details.json\r\n",
        "\r\n",
        "**Databases and tables created:**\r\n",
        "\r\n",
        "1. Spark DB: graphapi\r\n",
        "- Table: teams_activity_user_details"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfTeamsUserActivityRaw = dfTeamsUserActivityRaw.select(explode('value').alias('exploded_values')).select(\"exploded_values.*\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write Data Back to Lake"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write back to the lake in stage 2 ds2_main directory\r\n",
        "dfTeamsUserActivityRaw.write.format('parquet').mode('overwrite').save(stage2np + '/GraphAPI/teams_activity_user_details')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load to Spark DB"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark db to allow for access to the data in the delta-lake via SQL on-demand.\r\n",
        "# This is only creating metadata for SQL on-demand, pointing to the data in the delta-lake.\r\n",
        "# This also makes it possible to connect in Power BI via the azure sql data source connector.\r\n",
        "def create_spark_db(db_name, source_path):\r\n",
        "    spark.sql(f'CREATE DATABASE IF NOT EXISTS {db_name}')\r\n",
        "    spark.sql(f\"DROP TABLE IF EXISTS {db_name}.teams_activity_user_details\")\r\n",
        "    spark.sql(f\"create table if not exists {db_name}.users using PARQUET location '{source_path}/teams_activity_user_details'\")\r\n",
        "    \r\n",
        "create_spark_db('graphapi', stage2np + '/GraphAPI/teams_activity_user_details')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}